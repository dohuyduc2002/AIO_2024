{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-02T14:01:43.583007Z","iopub.status.busy":"2024-07-02T14:01:43.582738Z","iopub.status.idle":"2024-07-02T14:01:44.619847Z","shell.execute_reply":"2024-07-02T14:01:44.618885Z","shell.execute_reply.started":"2024-07-02T14:01:43.582982Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/test-pdf-for-rag/IELTS sample.pdf\n","/kaggle/input/test-pdf-for-rag/YOLOv10_Tutorials.pdf\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["# RAG with langchain"]},{"cell_type":"markdown","metadata":{},"source":["### Install dependancies"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T14:01:44.622097Z","iopub.status.busy":"2024-07-02T14:01:44.621618Z","iopub.status.idle":"2024-07-02T14:04:22.270971Z","shell.execute_reply":"2024-07-02T14:04:22.269665Z","shell.execute_reply.started":"2024-07-02T14:01:44.622062Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 24.4.1 requires cubinlinker, which is not installed.\n","cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cudf 24.4.1 requires ptxcompiler, which is not installed.\n","cuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","dask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n","keras-cv 0.9.0 requires keras-core, which is not installed.\n","keras-nlp 0.12.1 requires keras-core, which is not installed.\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n","apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n","apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n","apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\n","cudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\n","distributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\n","google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\n","jupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n","momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n","osmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\n","rapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\n","rapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\n","spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n","tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\n","ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n","distributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\n","kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n","kfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\n","rapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\n","rapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\n","tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\n","ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n","kfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 30.1.0 which is incompatible.\n","kfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q transformers==4.41.2\n","!pip install -q bitsandbytes==0.43.1\n","!pip install -q accelerate==0.31.0\n","!pip install -q langchain==0.2.5\n","!pip install -q langchainhub==0.1.20\n","!pip install -q langchain-chroma==0.1.1\n","!pip install -q langchain-community==0.2.5\n","!pip install -q langchain_huggingface==0.0.3\n","!pip install -q python-dotenv==1.0.1\n","!pip install -q pypdf==4.2.0"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T14:04:22.272954Z","iopub.status.busy":"2024-07-02T14:04:22.272592Z","iopub.status.idle":"2024-07-02T14:04:41.091406Z","shell.execute_reply":"2024-07-02T14:04:41.090407Z","shell.execute_reply.started":"2024-07-02T14:04:22.272917Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-07-02 14:04:27.992094: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-02 14:04:27.992206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-02 14:04:28.111505: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import torch\n","\n","from transformers import BitsAndBytesConfig\n","from transformers import AutoTokenizer,AutoModelForCausalLM,pipeline\n","from langchain_huggingface import HuggingFaceEmbeddings\n","from langchain_huggingface.llms import HuggingFacePipeline\n","\n","from langchain.memory import ConversationBufferMemory\n","from langchain_community.chat_message_histories import ChatMessageHistory\n","from langchain_community.document_loaders import PyPDFLoader,TextLoader\n","from langchain.chains import ConversationalRetrievalChain\n","\n","from langchain_chroma import Chroma\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain import hub"]},{"cell_type":"markdown","metadata":{},"source":["### Load testing data"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T14:04:41.094220Z","iopub.status.busy":"2024-07-02T14:04:41.093647Z","iopub.status.idle":"2024-07-02T14:04:41.998290Z","shell.execute_reply":"2024-07-02T14:04:41.997278Z","shell.execute_reply.started":"2024-07-02T14:04:41.094171Z"},"trusted":true},"outputs":[],"source":["Loader = PyPDFLoader\n","FILE_PATH = \"/kaggle/input/test-pdf-for-rag/YOLOv10_Tutorials.pdf\"\n","loader = Loader(FILE_PATH)\n","documents = loader.load()"]},{"cell_type":"markdown","metadata":{},"source":["### Splitting text in the PDF file"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T14:04:41.999876Z","iopub.status.busy":"2024-07-02T14:04:41.999510Z","iopub.status.idle":"2024-07-02T14:04:42.005905Z","shell.execute_reply":"2024-07-02T14:04:42.004983Z","shell.execute_reply.started":"2024-07-02T14:04:41.999835Z"},"trusted":true},"outputs":[],"source":["text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000 ,\n","                                                chunk_overlap=100)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T14:04:42.007605Z","iopub.status.busy":"2024-07-02T14:04:42.007315Z","iopub.status.idle":"2024-07-02T14:04:42.017329Z","shell.execute_reply":"2024-07-02T14:04:42.016422Z","shell.execute_reply.started":"2024-07-02T14:04:42.007580Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of sub - documents :  33\n","page_content='AI VIET NAM – AI COURSE 2024\\nTutorial: Phát hiện đối tượng trong ảnh với\\nYOLOv10\\nDinh-Thang Duong, Nguyen-Thuan Duong, Minh-Duc Bui và\\nQuang-Vinh Dinh\\nNgày 20 tháng 6 năm 2024\\nI. Giới thiệu\\nObject Detection (Tạm dịch: Phát hiện đối tượng) là một bài toán cổ điển thuộc lĩnh vực\\nComputer Vision. Mục tiêu của bài toán này là tự động xác định vị trí của các đối tượng trong\\nmột tấm ảnh. Tính tới thời điểm hiện tại, đã có rất nhiều phương pháp được phát triển nhằm\\ngiải quyết hiệu quả bài toán này. Trong đó, các phương pháp thuộc họ YOLO (You Only Look\\nOnce) thu hút được sự chú ý rất lớn từ cộng đồng nghiên cứu bởi độ chính xác và tốc độ thực\\nthi mà loại mô hình này mang lại.\\nHình 1: Logo của mô hình YOLO. Ảnh: link.\\nThời gian vừa qua, Ao Wang và các cộng sự tại Đại học Thanh Hoa (Tsinghua University)\\nđã đề xuất mô hình YOLOv10 trong bài báo YOLOv10: Real-Time End-to-End Object\\nDetection [10]. Với những cải tiến mới, mô hình đã đạt được hiệu suất vượt trội hơn so với các' metadata={'source': '/kaggle/input/test-pdf-for-rag/YOLOv10_Tutorials.pdf', 'page': 0}\n"]}],"source":["docs = text_splitter.split_documents(documents)\n","\n","print(\"Number of sub - documents : \",len(docs))\n","print(docs[0])"]},{"cell_type":"markdown","metadata":{},"source":["### Get model embedding"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T14:04:42.018883Z","iopub.status.busy":"2024-07-02T14:04:42.018562Z","iopub.status.idle":"2024-07-02T14:04:47.751235Z","shell.execute_reply":"2024-07-02T14:04:47.750230Z","shell.execute_reply.started":"2024-07-02T14:04:42.018855Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df89a0ce631748b9a24736837bcf897d","version_major":2,"version_minor":0},"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c7c972797594674804d3b7ef9b91a59","version_major":2,"version_minor":0},"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b219bc0c26a145809b98677348e5d5ec","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0ef9c2b8cabf45aaa6ee8474f23e6f89","version_major":2,"version_minor":0},"text/plain":["sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce1d4018224144b6bd09c755cd1f531d","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee24fea9327a4d34b842c75d86a0181c","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca0021dbdd5047008a4d4d8d4888d429","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a94c80f16254d7589d1fb54eb9ddc7a","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d28021beaf34bf2a937c4dee70eff08","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eedb866d879a49eba8f6b08f93153397","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1223bd5b647846cb92f979513a109107","version_major":2,"version_minor":0},"text/plain":["1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["embedding = HuggingFaceEmbeddings()"]},{"cell_type":"markdown","metadata":{},"source":["### Create a vector DB using Chroma"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T14:04:47.753323Z","iopub.status.busy":"2024-07-02T14:04:47.752538Z","iopub.status.idle":"2024-07-02T14:04:49.081952Z","shell.execute_reply":"2024-07-02T14:04:49.081152Z","shell.execute_reply.started":"2024-07-02T14:04:47.753287Z"},"trusted":true},"outputs":[],"source":["vector_db = Chroma.from_documents(documents=docs,\n","                                 embedding=embedding)\n","retriever = vector_db.as_retriever()"]},{"cell_type":"markdown","metadata":{},"source":["### Invoke knowledge from the vectorDB"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T14:04:49.083273Z","iopub.status.busy":"2024-07-02T14:04:49.082980Z","iopub.status.idle":"2024-07-02T14:04:49.121666Z","shell.execute_reply":"2024-07-02T14:04:49.120799Z","shell.execute_reply.started":"2024-07-02T14:04:49.083238Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of revevant documents 4\n"]}],"source":["result = retriever.invoke('What is YOLO')\n","print('Number of revevant documents', len(result))"]},{"cell_type":"markdown","metadata":{},"source":["### Model Quantization"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T14:04:49.125616Z","iopub.status.busy":"2024-07-02T14:04:49.125329Z","iopub.status.idle":"2024-07-02T14:04:49.175636Z","shell.execute_reply":"2024-07-02T14:04:49.174765Z","shell.execute_reply.started":"2024-07-02T14:04:49.125591Z"},"trusted":true},"outputs":[],"source":["nf4_config = BitsAndBytesConfig(\n","    load_in_4bit = True,\n","    bnb_4bit_quant_type='nf4',\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T14:04:49.176902Z","iopub.status.busy":"2024-07-02T14:04:49.176654Z","iopub.status.idle":"2024-07-02T14:05:56.982772Z","shell.execute_reply":"2024-07-02T14:05:56.981680Z","shell.execute_reply.started":"2024-07-02T14:04:49.176880Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cde529fc2c7445edb216c03708fb9e85","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bcae0ad11d974cb091d394e522e64492","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1df9473fbe8142899dd4c7fc6630ac04","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae2dba735cbe49198a73d7a6c4827185","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce910e7a358447b283643165552d96a2","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"755232eb99e541048e706b3d276aa506","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d5ee4b202304fc091b2b50507df5403","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/162 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"777cb4e873cd4b3a824e4724e272f5ab","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cb8609438f1f4e449ff1f4ae79290026","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"147440387b0647ef98a59bf5d954e7a3","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["MODEL_NAME = \"lmsys/vicuna-7b-v1.5\"\n","\n","model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config = nf4_config,\n","                                              low_cpu_mem_usage = True)\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T14:05:56.984483Z","iopub.status.busy":"2024-07-02T14:05:56.984097Z","iopub.status.idle":"2024-07-02T14:05:56.994833Z","shell.execute_reply":"2024-07-02T14:05:56.993942Z","shell.execute_reply.started":"2024-07-02T14:05:56.984450Z"},"trusted":true},"outputs":[],"source":["model_pipeline=pipeline(\n","    'text-generation',\n","    model = model,\n","    tokenizer = tokenizer,\n","    max_new_tokens=512,\n","    pad_token_id=tokenizer.eos_token_id,\n","    device_map='auto')\n","\n","llm = HuggingFacePipeline(pipeline=model_pipeline)"]},{"cell_type":"markdown","metadata":{},"source":["### Run the program"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T14:05:56.996592Z","iopub.status.busy":"2024-07-02T14:05:56.996242Z","iopub.status.idle":"2024-07-02T14:06:22.433706Z","shell.execute_reply":"2024-07-02T14:06:22.432751Z","shell.execute_reply.started":"2024-07-02T14:05:56.996561Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["YOLOv10 là một phiên bản của YOLO (You Only Look Once) - một hệ thống dự đoán hình ảnh được huấn luyện sẵn trên bộ dữ liệu COCO. Phiên bản này được tạo bằng cách tải về trọng số (file.pt) từ GitHub và khởi tạo mô hình bằng cách sử dụng thư viện ultralytics.\n"]}],"source":["\n","\n","prompt = hub.pull(\"rlm/rag-prompt\")\n","\n","def format_docs(docs):\n","  return \"\\n\\n\".join([doc.page_content for doc in docs])\n","\n","rag_chain = (\n","    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","USER_QUESTION = 'YOLOv10 là gì?'\n","output = rag_chain.invoke(USER_QUESTION)\n","# output = rag_chain.invoke({\"question\": USER_QUESTION})\n","answer = output.split(\"Answer:\")[1].strip()\n","print(answer)"]},{"cell_type":"markdown","metadata":{},"source":["# Developing UI using Chainlit and operating local with ngrok"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T14:06:22.435178Z","iopub.status.busy":"2024-07-02T14:06:22.434887Z","iopub.status.idle":"2024-07-02T14:09:05.979808Z","shell.execute_reply":"2024-07-02T14:09:05.978687Z","shell.execute_reply.started":"2024-07-02T14:06:22.435153Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 24.4.1 requires cubinlinker, which is not installed.\n","cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cudf 24.4.1 requires ptxcompiler, which is not installed.\n","cuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","dask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n","keras-cv 0.9.0 requires keras-core, which is not installed.\n","keras-nlp 0.12.1 requires keras-core, which is not installed.\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n","cudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\n","distributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\n","google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\n","jupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n","momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n","opentelemetry-instrumentation-asgi 0.43b0 requires opentelemetry-instrumentation==0.43b0, but you have opentelemetry-instrumentation 0.46b0 which is incompatible.\n","opentelemetry-instrumentation-asgi 0.43b0 requires opentelemetry-semantic-conventions==0.43b0, but you have opentelemetry-semantic-conventions 0.46b0 which is incompatible.\n","opentelemetry-instrumentation-fastapi 0.43b0 requires opentelemetry-instrumentation==0.43b0, but you have opentelemetry-instrumentation 0.46b0 which is incompatible.\n","opentelemetry-instrumentation-fastapi 0.43b0 requires opentelemetry-semantic-conventions==0.43b0, but you have opentelemetry-semantic-conventions 0.46b0 which is incompatible.\n","osmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\n","rapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\n","rapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\n","spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n","tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\n","ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\n","ypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 23.2.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[K\u001b[?25hm#################\u001b[0m\u001b[100;90m.\u001b[0m] / reify:debug: \u001b[32;40mhttp\u001b[0m \u001b[35mfetch\u001b[0m GET 200 https://registry.npmjs.o\u001b[0m\u001b[K\n","added 22 packages in 1s\n","\n","3 packages are looking for funding\n","  run `npm fund` for details\n","\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m \n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m New \u001b[33mminor\u001b[39m version of npm available! \u001b[31m10.1.0\u001b[39m -> \u001b[32m10.8.1\u001b[39m\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m Changelog: \u001b[36mhttps://github.com/npm/cli/releases/tag/v10.8.1\u001b[39m\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m Run \u001b[32mnpm install -g npm@10.8.1\u001b[39m to update!\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m \n","\u001b[0m"]}],"source":["!pip install -q transformers==4.41.2\n","!pip install -q bitsandbytes==0.43.1\n","!pip install -q accelerate==0.31.0\n","!pip install -q langchain==0.2.5\n","!pip install -q langchainhub==0.1.20\n","!pip install -q langchain-chroma==0.1.1\n","!pip install -q langchain-community==0.2.5\n","!pip install -q langchain-openai==0.1.9\n","!pip install -q langchain_huggingface==0.0.3\n","!pip install -q chainlit==1.1.304\n","!pip install -q python-dotenv==1.0.1\n","!pip install -q pypdf==4.2.0\n","!npm install -g localtunnel"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T15:16:01.801497Z","iopub.status.busy":"2024-07-02T15:16:01.801057Z","iopub.status.idle":"2024-07-02T15:16:01.811148Z","shell.execute_reply":"2024-07-02T15:16:01.810172Z","shell.execute_reply.started":"2024-07-02T15:16:01.801451Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting app.py\n"]}],"source":["%%writefile app.py\n","import chainlit as cl\n","import torch\n","\n","from chainlit.types import AskFileResponse\n","\n","from transformers import BitsAndBytesConfig\n","from transformers import AutoTokenizer , AutoModelForCausalLM , pipeline\n","from langchain_huggingface.llms import HuggingFacePipeline\n","\n","from langchain.memory import ConversationBufferMemory\n","from langchain_community.chat_message_histories import ChatMessageHistory\n","from langchain.chains import ConversationalRetrievalChain\n","\n","from langchain_huggingface import HuggingFaceEmbeddings\n","from langchain_chroma import Chroma\n","from langchain_community.document_loaders import PyPDFLoader, TextLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain import hub\n","\n","text_splitter = RecursiveCharacterTextSplitter (chunk_size =1000 ,chunk_overlap =100)\n","embedding = HuggingFaceEmbeddings ()\n","\n","def process_file(file: AskFileResponse):\n","    if file.type == 'text/plain':\n","        loader = TextLoader\n","    elif file.type == 'application/pdf':\n","        loader = PyPDFLoader\n","        \n","    loader = loader(file.path)\n","    documents = loader.load()\n","    docs = text_splitter.split_documents(documents)\n","    for i,doc in enumerate(docs):\n","        doc.metadata['source'] = f'source_{i}'\n","    return docs\n","\n","def get_vector_db(file: AskFileResponse):\n","    docs = process_file(file)\n","    cl.user_session.set('docs',docs)\n","    vector_db = Chroma.from_documents(\n","        documents = docs,\n","        embedding = embedding\n","    )\n","    return vector_db\n","\n","def get_huggingface_llm(model_name: str = 'lmsys/vicuna-7b-v1.5',\n","                        max_new_token: int = 512):\n","    nf4_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_quant_type='nf4',\n","        bnb4bit_use_double_quant=True,\n","        bnb_4bit_compute_dtype=torch.bfloat16\n","    )\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_name,\n","        quantization_config=nf4_config,\n","        low_cpu_mem_usage=True\n","    )\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","    model_pipeline = pipeline(\n","        'text-generation',\n","        model=model,\n","        tokenizer=tokenizer,\n","        max_new_tokens=max_new_token,\n","        pad_token_id=tokenizer.eos_token_id,\n","        device_map='auto'\n","    )\n","    llm = HuggingFacePipeline(pipeline=model_pipeline)\n","    return llm\n","\n","LLM = get_huggingface_llm()\n","\n","welcome_message = \"\"\" Welcome to the PDF QA! To get started :\n","1.Upload a PDF or text file\n","2.Ask a question about the file\n","\"\"\"\n","\n","@cl.on_chat_start\n","async def on_chat_start():\n","    files = None\n","    while files is None:\n","        files = await cl.AskFileMessage(\n","            content = welcome_message,\n","            accept=['text/plain','application/pdf'],\n","            max_size_mb = 20,\n","            timeout=180\n","        ).send()\n","        file = files[0]\n","\n","        msg = cl.Message(content = f'Processing file {file.name}...',\n","                         disable_feedback=True)\n","        await msg.send()\n","\n","        vector_db = await cl.make_async(get_vector_db)(file)\n","\n","        message_history = ChatMessageHistory()\n","        memory = ConversationBufferMemory(memory_key='chat_history',\n","                                          output_key = 'answer',\n","                                          chat_memory = message_history,\n","                                          return_messages = True)\n","        retriever = vector_db.as_retriever(search_type = 'mmr',search_kwargs = {'k':3})\n","        chain = ConversationalRetrievalChain.from_llm(llm = LLM,\n","                                                      chain_type = 'stuff',\n","                                                      retriever = retriever,\n","                                                      memory = memory,return_source_documents = True)\n","        msg.content = f\"'{file.name} processed. You can now ask question!\"\n","        await msg.update()\n","\n","        cl.user_session.set('chain',chain)\n","        \n","@cl.on_message\n","async def on_message(message:cl.Message):\n","    chain = cl.user_session.get(\"chain\")\n","    cb = cl.AsyncLangchainCallbackHandler ()\n","    res = await chain.ainvoke(message.content,callbacks =[ cb ])\n","    answer = res[\"answer\"]\n","    source_documents = res[\"source_documents\"]\n","    text_elements = []\n","\n","    if source_documents :\n","        for source_idx , source_doc in enumerate(source_documents) :\n","            source_name = f\"source_{source_idx}\"\n","            text_elements.append(\n","                cl.Text( content = source_doc.page_content ,name = source_name))\n","    source_names = [text_el.name for text_el in text_elements ]\n","    if source_names :\n","        answer += f\"\\ nSources : {' ,'. join ( source_names )}\"\n","    else :\n","        answer += \"\\nNo sources found \"\n","    await cl.Message(content = answer ,elements = text_elements).send()"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T15:16:01.813580Z","iopub.status.busy":"2024-07-02T15:16:01.813234Z","iopub.status.idle":"2024-07-02T15:16:14.146044Z","shell.execute_reply":"2024-07-02T15:16:14.144748Z","shell.execute_reply.started":"2024-07-02T15:16:01.813547Z"},"trusted":true},"outputs":[],"source":["!pip install pyngrok -q"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T15:16:14.147823Z","iopub.status.busy":"2024-07-02T15:16:14.147518Z","iopub.status.idle":"2024-07-02T15:16:15.615399Z","shell.execute_reply":"2024-07-02T15:16:15.614286Z","shell.execute_reply.started":"2024-07-02T15:16:14.147794Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n","https://7ae3-35-230-100-249.ngrok-free.app\n"]}],"source":["from pyngrok import ngrok\n","# !ngrok config add-authtoken <SECRET NGROK AUTHENTICATION KEY>\n","public_url = ngrok.connect(8000).public_url\n","print(public_url)"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T15:16:15.617022Z","iopub.status.busy":"2024-07-02T15:16:15.616761Z","iopub.status.idle":"2024-07-02T15:22:16.973044Z","shell.execute_reply":"2024-07-02T15:22:16.972142Z","shell.execute_reply.started":"2024-07-02T15:16:15.616996Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-07-02 15:16:21.142370: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-02 15:16:21.142425: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-02 15:16:21.143889: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Unused kwargs: ['bnb4bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n","Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.45s/it]\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","^C\n"]}],"source":["!chainlit run app.py"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5307055,"sourceId":8821612,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
