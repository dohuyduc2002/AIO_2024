{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e5efe35",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-21T15:34:09.890180Z",
     "iopub.status.busy": "2025-01-21T15:34:09.889869Z",
     "iopub.status.idle": "2025-01-21T15:34:10.562106Z",
     "shell.execute_reply": "2025-01-21T15:34:10.561153Z"
    },
    "papermill": {
     "duration": 0.679928,
     "end_time": "2025-01-21T15:34:10.563861",
     "exception": false,
     "start_time": "2025-01-21T15:34:09.883933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae5a81c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T15:34:10.573358Z",
     "iopub.status.busy": "2025-01-21T15:34:10.573030Z",
     "iopub.status.idle": "2025-01-21T15:34:19.641511Z",
     "shell.execute_reply": "2025-01-21T15:34:19.640607Z"
    },
    "papermill": {
     "duration": 9.074545,
     "end_time": "2025-01-21T15:34:19.642964",
     "exception": false,
     "start_time": "2025-01-21T15:34:10.568419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from collections import Counter\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "983b2ed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T15:34:19.652835Z",
     "iopub.status.busy": "2025-01-21T15:34:19.652427Z",
     "iopub.status.idle": "2025-01-21T15:34:19.660656Z",
     "shell.execute_reply": "2025-01-21T15:34:19.659965Z"
    },
    "papermill": {
     "duration": 0.014292,
     "end_time": "2025-01-21T15:34:19.661819",
     "exception": false,
     "start_time": "2025-01-21T15:34:19.647527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CustomVOCDataset(torchvision.datasets.VOCDetection):\n",
    "    def init_config_yolo(self, class_mapping, S=7, B=2, C=20, custom_transforms=None):\n",
    "        \"\"\"\n",
    "        Initialize YOLO-specific configuration parameters.\n",
    "        Args:\n",
    "            class_mapping (dict): Mapping of class names to indices.\n",
    "            S (int): Grid size (S x S).\n",
    "            B (int): Number of bounding boxes per grid cell.\n",
    "            C (int): Number of classes.\n",
    "            custom_transforms (callable): Optional transformations for the dataset.\n",
    "        \"\"\"\n",
    "        self.S = S  # Grid size S x S\n",
    "        self.B = B  # Number of bounding boxes\n",
    "        self.C = C  # Number of classes\n",
    "        self.class_mapping = class_mapping  # Class-to-index mapping\n",
    "        self.custom_transforms = custom_transforms  # Custom transformations\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get an image and its corresponding YOLO-style label matrix.\n",
    "        \"\"\"\n",
    "        # Get an image and its target (annotations) from the VOC dataset\n",
    "        image, target = super(CustomVOCDataset, self).__getitem__(index)\n",
    "        img_width, img_height = image.size\n",
    "\n",
    "        # Convert target annotations to YOLO format bounding boxes\n",
    "        boxes = convert_to_yolo_format(target, img_width, img_height, self.class_mapping)\n",
    "\n",
    "        # Separate boxes and labels\n",
    "        just_boxes = boxes[:, 1:]\n",
    "        labels = boxes[:, 0]\n",
    "\n",
    "        # Apply custom transformations if provided\n",
    "        if self.custom_transforms:\n",
    "            sample = {\n",
    "                'image': np.array(image),\n",
    "                'bboxes': just_boxes,\n",
    "                'labels': labels,\n",
    "            }\n",
    "            sample = self.custom_transforms(**sample)\n",
    "            image = sample['image']\n",
    "            boxes = sample['bboxes']\n",
    "            labels = sample['labels']\n",
    "\n",
    "        # Create an empty label matrix for YOLO ground truth\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "\n",
    "        # Convert boxes and labels to PyTorch tensors\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        image = torch.as_tensor(image, dtype=torch.float32)\n",
    "\n",
    "        # Iterate through each bounding box in YOLO format\n",
    "        for box, class_label in zip(boxes, labels):\n",
    "            x, y, width, height = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "\n",
    "            # Calculate the grid cell (i, j) that this box belongs to\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "            # Calculate the width and height of the box relative to the grid cell\n",
    "            width_cell, height_cell = width * self.S, height * self.S\n",
    "\n",
    "            # If no object has been found in this specific cell (i, j) before\n",
    "            if label_matrix[i, j, 20] == 0:\n",
    "                # Mark that an object exists in this cell\n",
    "                label_matrix[i, j, 20] = 1\n",
    "\n",
    "                # Store the box coordinates as an offset from the cell boundaries\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, width_cell, height_cell]\n",
    "                )\n",
    "\n",
    "                # Set the box coordinates in the label matrix\n",
    "                label_matrix[i, j, 21:25] = box_coordinates\n",
    "\n",
    "                # Set the one-hot encoding for the class label\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "\n",
    "        return image, label_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70948c21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T15:34:19.670833Z",
     "iopub.status.busy": "2025-01-21T15:34:19.670627Z",
     "iopub.status.idle": "2025-01-21T15:34:19.675890Z",
     "shell.execute_reply": "2025-01-21T15:34:19.675282Z"
    },
    "papermill": {
     "duration": 0.011114,
     "end_time": "2025-01-21T15:34:19.677061",
     "exception": false,
     "start_time": "2025-01-21T15:34:19.665947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_to_yolo_format(target, img_width, img_height, class_mapping):\n",
    "    \"\"\"\n",
    "    Convert annotation data from VOC format to YOLO format.\n",
    "\n",
    "    Parameters:\n",
    "        target (dict): Annotation data from VOCDetection dataset.\n",
    "        img_width (int): Width of the original image.\n",
    "        img_height (int): Height of the original image.\n",
    "        class_mapping (dict): Mapping from class names to integer IDs.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape [N, 5] for N bounding boxes,\n",
    "        each with [class_id, x_center, y_center, width, height].\n",
    "    \"\"\"\n",
    "    # Extract the list of annotations from the target dictionary\n",
    "    annotations = target['annotation']['object']\n",
    "\n",
    "    # Get the real width and height of the image from the annotation\n",
    "    real_width = int(target['annotation']['size']['width'])\n",
    "    real_height = int(target['annotation']['size']['height'])\n",
    "\n",
    "    # Ensure that annotations is a list, even if thereâ€™s only one object\n",
    "    if not isinstance(annotations, list):\n",
    "        annotations = [annotations]\n",
    "\n",
    "    # Initialize an empty list to store the converted bounding boxes\n",
    "    boxes = []\n",
    "\n",
    "    # Loop through each annotation and convert it to YOLO format\n",
    "    for anno in annotations:\n",
    "        xmin = int(anno['bndbox']['xmin']) / real_width\n",
    "        xmax = int(anno['bndbox']['xmax']) / real_width\n",
    "        ymin = int(anno['bndbox']['ymin']) / real_height\n",
    "        ymax = int(anno['bndbox']['ymax']) / real_height\n",
    "\n",
    "        # Calculate the center coordinates, width, and height of the bounding box\n",
    "        x_center = (xmin + xmax) / 2\n",
    "        y_center = (ymin + ymax) / 2\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "\n",
    "        # Retrieve the class name from the annotation and map it to an integer ID\n",
    "        class_name = anno['name']\n",
    "        class_id = class_mapping[class_name] if class_name in class_mapping else 0\n",
    "\n",
    "        # Append the YOLO formatted bounding box to the list\n",
    "        boxes.append([class_id, x_center, y_center, width, height])\n",
    "\n",
    "    # Convert the list of boxes to a numpy array\n",
    "    return np.array(boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "516cef21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T15:34:19.685748Z",
     "iopub.status.busy": "2025-01-21T15:34:19.685540Z",
     "iopub.status.idle": "2025-01-21T15:34:19.692283Z",
     "shell.execute_reply": "2025-01-21T15:34:19.691667Z"
    },
    "papermill": {
     "duration": 0.012313,
     "end_time": "2025-01-21T15:34:19.693410",
     "exception": false,
     "start_time": "2025-01-21T15:34:19.681097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) between bounding boxes.\n",
    "\n",
    "    Parameters:\n",
    "        boxes_preds (tensor): Predicted bounding boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): Ground truth bounding boxes (BATCH_SIZE, 4)\n",
    "        box_format (str): Box format, can be \"midpoint\" or \"corners\".\n",
    "\n",
    "    Returns:\n",
    "        tensor: Intersection over Union scores for each example.\n",
    "    \"\"\"\n",
    "    # If box format is \"midpoint\", convert to \"corners\" format\n",
    "    if box_format == \"midpoint\":\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    # If box format is \"corners\", use the coordinates directly\n",
    "    elif box_format == \"corners\":\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]\n",
    "\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "\n",
    "    # Calculate the coordinates of the intersection rectangle\n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    # Compute the area of the intersection rectangle, clamp(0) to handle non-overlapping boxes\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "\n",
    "    # Calculate the area of both bounding boxes\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "\n",
    "    # Calculate Intersection over Union (IoU)\n",
    "    iou = intersection / (box1_area + box2_area - intersection + 1e-6)\n",
    "\n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2032b9cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T15:34:19.702076Z",
     "iopub.status.busy": "2025-01-21T15:34:19.701881Z",
     "iopub.status.idle": "2025-01-21T15:34:19.706521Z",
     "shell.execute_reply": "2025-01-21T15:34:19.705919Z"
    },
    "papermill": {
     "duration": 0.010229,
     "end_time": "2025-01-21T15:34:19.707626",
     "exception": false,
     "start_time": "2025-01-21T15:34:19.697397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
    "    \"\"\"\n",
    "    Perform Non-Maximum Suppression (NMS) on a list of bounding boxes.\n",
    "\n",
    "    Parameters:\n",
    "        bboxes (list): List of bounding boxes, each represented as\n",
    "                       [class_pred, prob_score, x1, y1, x2, y2].\n",
    "        iou_threshold (float): IoU threshold to determine overlap for suppression.\n",
    "        threshold (float): Probability threshold to filter bounding boxes.\n",
    "        box_format (str): Format of bounding boxes, either \"midpoint\" or \"corners\".\n",
    "\n",
    "    Returns:\n",
    "        list: Bounding boxes after applying NMS.\n",
    "    \"\"\"\n",
    "    # Ensure bboxes is a list\n",
    "    assert isinstance(bboxes, list), \"bboxes must be a list\"\n",
    "\n",
    "    # Filter bounding boxes by probability threshold\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "\n",
    "    # Sort bounding boxes by probability score in descending order\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # List to store bounding boxes after NMS\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    # Perform NMS\n",
    "    while bboxes:\n",
    "        # Select the bounding box with the highest probability\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        # Remove bounding boxes with IoU greater than the threshold\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]  # Keep boxes of different classes\n",
    "            or intersection_over_union(\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "                box_format=box_format,\n",
    "            )\n",
    "            < iou_threshold  # Remove boxes with high IoU overlap\n",
    "        ]\n",
    "\n",
    "        # Add the chosen box to the result list\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfb39964",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T15:34:19.716231Z",
     "iopub.status.busy": "2025-01-21T15:34:19.716038Z",
     "iopub.status.idle": "2025-01-21T15:34:19.723964Z",
     "shell.execute_reply": "2025-01-21T15:34:19.723373Z"
    },
    "papermill": {
     "duration": 0.013467,
     "end_time": "2025-01-21T15:34:19.725086",
     "exception": false,
     "start_time": "2025-01-21T15:34:19.711619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the mean average precision (mAP).\n",
    "\n",
    "    Parameters:\n",
    "        pred_boxes (list): List of predicted bounding boxes, each defined as\n",
    "                           [train_idx, class_pred, prob_score, x1, y1, x2, y2].\n",
    "        true_boxes (list): List of ground truth bounding boxes, each defined as\n",
    "                           [train_idx, class_label, x1, y1, x2, y2].\n",
    "        iou_threshold (float): IoU threshold to consider a prediction as correct.\n",
    "        box_format (str): \"midpoint\" or \"corners\" format for bounding boxes.\n",
    "        num_classes (int): Total number of classes.\n",
    "\n",
    "    Returns:\n",
    "        float: mAP value across all classes at the specified IoU threshold.\n",
    "    \"\"\"\n",
    "    # List to store average precision for each class\n",
    "    average_precisions = []\n",
    "\n",
    "    # Small epsilon to avoid division by zero\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        # Filter predictions and ground truths for the current class\n",
    "        detections = [detection for detection in pred_boxes if detection[1] == c]\n",
    "        ground_truths = [gt for gt in true_boxes if gt[1] == c]\n",
    "\n",
    "        # Count the number of ground truth boxes for each image\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # Sort detections by probability in descending order\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        # Initialize True Positive (TP) and False Positive (FP) arrays\n",
    "        TP = torch.zeros(len(detections))\n",
    "        FP = torch.zeros(len(detections))\n",
    "\n",
    "        # Total ground truth boxes for this class\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "\n",
    "        # Skip if there are no ground truth boxes for this class\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Get all ground truth boxes for the same image as the detection\n",
    "            ground_truth_img = [gt for gt in ground_truths if gt[0] == detection[0]]\n",
    "\n",
    "            best_iou = 0\n",
    "            best_gt_idx = -1\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                    box_format=box_format,\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            # If IoU is greater than the threshold, mark as True Positive\n",
    "            if best_iou > iou_threshold:\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        # Cumulative sum of True Positives and False Positives\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "\n",
    "        # Recall and Precision\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
    "\n",
    "        # Add a starting point for recalls and precisions\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "\n",
    "        # Average Precision for this class (using trapezoidal rule for integration)\n",
    "        average_precision = torch.trapz(precisions, recalls)\n",
    "        average_precisions.append(average_precision)\n",
    "\n",
    "    # Return the mean of average precisions across all classes\n",
    "    return sum(average_precisions) / len(average_precisions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbd1a146",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T15:34:19.733848Z",
     "iopub.status.busy": "2025-01-21T15:34:19.733644Z",
     "iopub.status.idle": "2025-01-21T15:34:19.744765Z",
     "shell.execute_reply": "2025-01-21T15:34:19.744118Z"
    },
    "papermill": {
     "duration": 0.016847,
     "end_time": "2025-01-21T15:34:19.745950",
     "exception": false,
     "start_time": "2025-01-21T15:34:19.729103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# YOLOv1 architecture configuration\n",
    "architecture_config = [\n",
    "    (7, 64, 2, 3),  # Convolutional block 1\n",
    "    \"M\",            # Max-pooling layer 1\n",
    "    (3, 192, 1, 1),  # Convolutional block 2\n",
    "    \"M\",            # Max-pooling layer 2\n",
    "    (1, 128, 1, 0),  # Convolutional block 3\n",
    "    (3, 256, 1, 1),  # Convolutional block 4\n",
    "    (1, 256, 1, 0),  # Convolutional block 5\n",
    "    (3, 512, 1, 1),  # Convolutional block 6\n",
    "    \"M\",            # Max-pooling layer 3\n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],  # Convolutional block 7 (repeated 4 times)\n",
    "    (1, 512, 1, 0),  # Convolutional block 8\n",
    "    (3, 1024, 1, 1), # Convolutional block 9\n",
    "    \"M\",            # Max-pooling layer 4\n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],  # Convolutional block 10 (repeated 2 times)\n",
    "    (3, 1024, 1, 1), # Convolutional block 11\n",
    "    (3, 1024, 2, 1), # Convolutional block 12\n",
    "    (3, 1024, 1, 1), # Convolutional block 13\n",
    "    (3, 1024, 1, 1), # Convolutional block 14\n",
    "]\n",
    "\n",
    "# A convolutional block with Conv2d, BatchNorm2d, and LeakyReLU layers\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "\n",
    "# YOLOv1 model class\n",
    "class Yolov1(nn.Module):\n",
    "    def __init__(self, in_channels=3, **kwargs):\n",
    "        super(Yolov1, self).__init__()\n",
    "        self.architecture = architecture_config\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        return self.fcs(torch.flatten(x, start_dim=1))\n",
    "\n",
    "    def _create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            if type(x) == tuple:\n",
    "                layers += [\n",
    "                    CNNBlock(\n",
    "                        in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3],\n",
    "                    )\n",
    "                ]\n",
    "                in_channels = x[1]\n",
    "\n",
    "            elif type(x) == str:\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n",
    "\n",
    "            elif type(x) == list:\n",
    "                conv1 = x[0]\n",
    "                conv2 = x[1]\n",
    "                num_repeats = x[2]\n",
    "\n",
    "                for _ in range(num_repeats):\n",
    "                    layers += [\n",
    "                        CNNBlock(\n",
    "                            in_channels,\n",
    "                            conv1[1],\n",
    "                            kernel_size=conv1[0],\n",
    "                            stride=conv1[2],\n",
    "                            padding=conv1[3],\n",
    "                        )\n",
    "                    ]\n",
    "                    layers += [\n",
    "                        CNNBlock(\n",
    "                            conv1[1],\n",
    "                            conv2[1],\n",
    "                            kernel_size=conv2[0],\n",
    "                            stride=conv2[2],\n",
    "                            padding=conv2[3],\n",
    "                        )\n",
    "                    ]\n",
    "                    in_channels = conv2[1]\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * S * S, 4096),\n",
    "            nn.Dropout(0.0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(4096, S * S * (C + B * 5)),  # Final output layer\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a58504",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T15:34:19.754573Z",
     "iopub.status.busy": "2025-01-21T15:34:19.754322Z",
     "iopub.status.idle": "2025-01-21T15:34:19.763796Z",
     "shell.execute_reply": "2025-01-21T15:34:19.763165Z"
    },
    "papermill": {
     "duration": 0.014953,
     "end_time": "2025-01-21T15:34:19.764873",
     "exception": false,
     "start_time": "2025-01-21T15:34:19.749920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for the YOLO (v1) model.\n",
    "    \"\"\"\n",
    "    def __init__(self, S=7, B=2, C=20):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")  # Mean Squared Error loss with sum reduction\n",
    "        self.S = S  # Grid size\n",
    "        self.B = B  # Number of bounding boxes\n",
    "        self.C = C  # Number of classes\n",
    "        self.lambda_noobj = 0.5  # Weight for no object loss\n",
    "        self.lambda_coord = 5  # Weight for box coordinate loss\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        \"\"\"\n",
    "        Compute YOLOv1 loss.\n",
    "\n",
    "        Args:\n",
    "            predictions: Tensor of shape (BATCH_SIZE, S*S*(C + B*5)).\n",
    "            target: Tensor of same shape as predictions.\n",
    "\n",
    "        Returns:\n",
    "            loss: Total loss (scalar).\n",
    "        \"\"\"\n",
    "        # Reshape predictions to (BATCH_SIZE, S, S, C + B*5)\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        # Calculate IoU for the two predicted bounding boxes with the target box\n",
    "        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n",
    "        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        # Determine which bounding box has the highest IoU\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        exists_box = target[..., 20].unsqueeze(3)  # Indicator for object presence\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "        box_predictions = exists_box * (\n",
    "            bestbox * predictions[..., 26:30] + (1 - bestbox) * predictions[..., 21:25]\n",
    "        )\n",
    "        box_targets = exists_box * target[..., 21:25]\n",
    "\n",
    "        # Square root of width and height to stabilize gradients\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4]) + 1e-6\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21]\n",
    "        )\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., 20:21]),\n",
    "        )\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        )\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :20], end_dim=-2),\n",
    "            torch.flatten(exists_box * target[..., :20], end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   TOTAL LOSS       #\n",
    "        # ================== #\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss  # Box coordinate loss\n",
    "            + object_loss                 # Object presence loss\n",
    "            + self.lambda_noobj * no_object_loss  # No object loss\n",
    "            + class_loss                  # Classification loss\n",
    "        )\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2825f821",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T15:34:19.773396Z",
     "iopub.status.busy": "2025-01-21T15:34:19.773195Z",
     "iopub.status.idle": "2025-01-21T15:34:19.780577Z",
     "shell.execute_reply": "2025-01-21T15:34:19.779976Z"
    },
    "papermill": {
     "duration": 0.012822,
     "end_time": "2025-01-21T15:34:19.781643",
     "exception": false,
     "start_time": "2025-01-21T15:34:19.768821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Hyperparameters and configurations\n",
    "LEARNING_RATE = 2e-5  # Learning rate for the optimizer\n",
    "DEVICE = \"cuda\"  # Use \"cuda\" for GPU or \"cpu\" for CPU\n",
    "BATCH_SIZE = 16  # Batch size (originally 64 in the paper, reduced for GPU limitations)\n",
    "EPOCHS = 300  # Number of training epochs\n",
    "NUM_WORKERS = 2  # Number of worker processes for data loading\n",
    "PIN_MEMORY = True  # Pin memory for faster data transfer to GPU\n",
    "LOAD_MODEL = False  # Set to True to load a pre-trained model\n",
    "LOAD_MODEL_FILE = \"yolov1.pth.tar\"  # Pre-trained model file (if LOAD_MODEL is True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95d4fe70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T15:34:19.790073Z",
     "iopub.status.busy": "2025-01-21T15:34:19.789877Z",
     "iopub.status.idle": "2025-01-21T15:34:19.795243Z",
     "shell.execute_reply": "2025-01-21T15:34:19.794678Z"
    },
    "papermill": {
     "duration": 0.010873,
     "end_time": "2025-01-21T15:34:19.796390",
     "exception": false,
     "start_time": "2025-01-21T15:34:19.785517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Image dimensions\n",
    "WIDTH = 448\n",
    "HEIGHT = 448\n",
    "\n",
    "# Training transformations\n",
    "def get_train_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    A.HueSaturationValue(\n",
    "                        hue_shift_limit=0.2, \n",
    "                        sat_shift_limit=0.2, \n",
    "                        val_shift_limit=0.2, \n",
    "                        p=0.9\n",
    "                    ),\n",
    "                    A.RandomBrightnessContrast(\n",
    "                        brightness_limit=0.2, \n",
    "                        contrast_limit=0.2, \n",
    "                        p=0.9\n",
    "                    ),\n",
    "                ],\n",
    "                p=0.9,\n",
    "            ),\n",
    "            A.ToGray(p=0.01),\n",
    "            A.HorizontalFlip(p=0.2),\n",
    "            A.VerticalFlip(p=0.2),\n",
    "            A.Resize(height=HEIGHT, width=WIDTH, p=1.0),\n",
    "            # Uncomment if you want to use Cutout for data augmentation\n",
    "            # A.Cutout(\n",
    "            #     num_holes=8, \n",
    "            #     max_h_size=64, \n",
    "            #     max_w_size=64, \n",
    "            #     fill_value=0, \n",
    "            #     p=0.5\n",
    "            # ),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        p=1.0,\n",
    "        bbox_params=A.BboxParams(\n",
    "            format='yolo', \n",
    "            min_area=0, \n",
    "            min_visibility=0, \n",
    "            label_fields=['labels']\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# Validation transformations\n",
    "def get_valid_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=HEIGHT, width=WIDTH, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        p=1.0,\n",
    "        bbox_params=A.BboxParams(\n",
    "            format='yolo', \n",
    "            min_area=0, \n",
    "            min_visibility=0, \n",
    "            label_fields=['labels']\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be5f309e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T15:34:19.804963Z",
     "iopub.status.busy": "2025-01-21T15:34:19.804768Z",
     "iopub.status.idle": "2025-01-21T15:34:19.808108Z",
     "shell.execute_reply": "2025-01-21T15:34:19.807544Z"
    },
    "papermill": {
     "duration": 0.008808,
     "end_time": "2025-01-21T15:34:19.809157",
     "exception": false,
     "start_time": "2025-01-21T15:34:19.800349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    'aeroplane': 0,\n",
    "    'bicycle': 1,\n",
    "    'bird': 2,\n",
    "    'boat': 3,\n",
    "    'bottle': 4,\n",
    "    'bus': 5,\n",
    "    'car': 6,\n",
    "    'cat': 7,\n",
    "    'chair': 8,\n",
    "    'cow': 9,\n",
    "    'diningtable': 10,\n",
    "    'dog': 11,\n",
    "    'horse': 12,\n",
    "    'motorbike': 13,\n",
    "    'person': 14,\n",
    "    'pottedplant': 15,\n",
    "    'sheep': 16,\n",
    "    'sofa': 17,\n",
    "    'train': 18,\n",
    "    'tvmonitor': 19,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caa1fe65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T15:34:19.817800Z",
     "iopub.status.busy": "2025-01-21T15:34:19.817570Z",
     "iopub.status.idle": "2025-01-21T15:34:19.823360Z",
     "shell.execute_reply": "2025-01-21T15:34:19.822600Z"
    },
    "papermill": {
     "duration": 0.011506,
     "end_time": "2025-01-21T15:34:19.824621",
     "exception": false,
     "start_time": "2025-01-21T15:34:19.813115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from termcolor import colored\n",
    "\n",
    "def train_fn(train_loader, model, optimizer, loss_fn, epoch):\n",
    "    \"\"\"\n",
    "    Training function for YOLO.\n",
    "\n",
    "    Args:\n",
    "        train_loader: DataLoader for training data.\n",
    "        model: YOLO model.\n",
    "        optimizer: Optimizer for model training.\n",
    "        loss_fn: Loss function for YOLO.\n",
    "        epoch: Current epoch number.\n",
    "\n",
    "    Returns:\n",
    "        avg_mAP: Average mean Average Precision (mAP) for the epoch.\n",
    "    \"\"\"\n",
    "    mean_loss = []\n",
    "    mean_mAP = []\n",
    "\n",
    "    total_batches = len(train_loader)\n",
    "    display_interval = total_batches // 5  # Log progress at 20% intervals\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Generate predicted and true bounding boxes\n",
    "        pred_boxes, true_boxes = get_bboxes_training(\n",
    "            out, y, iou_threshold=0.5, threshold=0.4\n",
    "        )\n",
    "        mAP = mean_average_precision(\n",
    "            pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "        )\n",
    "\n",
    "        mean_loss.append(loss.item())\n",
    "        mean_mAP.append(mAP.item())\n",
    "\n",
    "        if batch_idx % display_interval == 0 or batch_idx == total_batches - 1:\n",
    "            print(\n",
    "                f\"Epoch: {epoch:3} \\t Iter: {batch_idx:3}/{total_batches:3} \\t\"\n",
    "                f\"Loss: {loss.item():3.10f} \\t mAP: {mAP.item():3.10f}\"\n",
    "            )\n",
    "\n",
    "    avg_loss = sum(mean_loss) / len(mean_loss)\n",
    "    avg_mAP = sum(mean_mAP) / len(mean_mAP)\n",
    "\n",
    "    print(colored(f\"Train \\t loss: {avg_loss:3.10f} \\t mAP: {avg_mAP:3.10f}\", \"green\"))\n",
    "\n",
    "    return avg_mAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "021f0fbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T15:34:19.833287Z",
     "iopub.status.busy": "2025-01-21T15:34:19.833065Z",
     "iopub.status.idle": "2025-01-21T15:34:19.841285Z",
     "shell.execute_reply": "2025-01-21T15:34:19.840518Z"
    },
    "papermill": {
     "duration": 0.013838,
     "end_time": "2025-01-21T15:34:19.842435",
     "exception": false,
     "start_time": "2025-01-21T15:34:19.828597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "from termcolor import colored\n",
    "\n",
    "def train():\n",
    "    # Initialize model, optimizer, and loss function\n",
    "    model = Yolov1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = YoloLoss()\n",
    "\n",
    "    # Load pre-trained model checkpoint if required\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
    "\n",
    "    # Create and configure training dataset\n",
    "    train_dataset = CustomVOCDataset(\n",
    "        root='./data',\n",
    "        year='2012',\n",
    "        image_set='train',\n",
    "        download=True,\n",
    "    )\n",
    "    train_dataset.init_config_yolo(class_mapping=class_mapping, custom_transforms=get_train_transforms())\n",
    "\n",
    "    # Create and configure validation and test datasets\n",
    "    testval_dataset = CustomVOCDataset(\n",
    "        root='./data',\n",
    "        year='2012',\n",
    "        image_set='val',\n",
    "        download=True,\n",
    "    )\n",
    "    testval_dataset.init_config_yolo(class_mapping=class_mapping, custom_transforms=get_valid_transforms())\n",
    "\n",
    "    # Split dataset into validation and test sets\n",
    "    dataset_size = len(testval_dataset)\n",
    "    val_size = int(0.15 * dataset_size)\n",
    "    test_size = dataset_size - val_size\n",
    "\n",
    "    val_indices = list(range(val_size))\n",
    "    test_indices = list(range(val_size, val_size + test_size))\n",
    "\n",
    "    # Create SubsetRandomSamplers for validation and test sets\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        dataset=testval_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        sampler=val_sampler,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=testval_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        sampler=test_sampler,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # Initialize best mAP trackers\n",
    "    best_mAP_train = 0\n",
    "    best_mAP_val = 0\n",
    "    best_mAP_test = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Train and evaluate on train, validation, and test sets\n",
    "        train_mAP = train_fn(train_loader, model, optimizer, loss_fn, epoch)\n",
    "        val_mAP = test_fn(val_loader, model, loss_fn, epoch)  # Using test_fn for validation\n",
    "        test_mAP = test_fn(test_loader, model, loss_fn, epoch)\n",
    "\n",
    "        # Update best mAP values\n",
    "        if train_mAP > best_mAP_train:\n",
    "            best_mAP_train = train_mAP\n",
    "        if val_mAP > best_mAP_val:\n",
    "            best_mAP_val = val_mAP\n",
    "            # Save checkpoint when validation mAP improves\n",
    "            checkpoint = {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "            }\n",
    "            save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n",
    "        if test_mAP > best_mAP_test:\n",
    "            best_mAP_test = test_mAP\n",
    "\n",
    "        # Print best mAP values\n",
    "        print(colored(f\"Best Train mAP: {best_mAP_train:3.10f}\", 'green'))\n",
    "        print(colored(f\"Best Val mAP: {best_mAP_val:3.10f}\", 'blue'))\n",
    "        print(colored(f\"Best Test mAP: {best_mAP_test:3.10f}\", 'yellow'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0df3c5d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T15:34:19.851263Z",
     "iopub.status.busy": "2025-01-21T15:34:19.851043Z",
     "iopub.status.idle": "2025-01-21T15:34:19.858217Z",
     "shell.execute_reply": "2025-01-21T15:34:19.857443Z"
    },
    "papermill": {
     "duration": 0.013004,
     "end_time": "2025-01-21T15:34:19.859385",
     "exception": false,
     "start_time": "2025-01-21T15:34:19.846381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def plot_image_with_labels(image, ground_truth_boxes, predicted_boxes, class_mapping):\n",
    "    \"\"\"\n",
    "    Plot an image with both ground truth and predicted bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        image: The image to plot (PyTorch tensor or NumPy array).\n",
    "        ground_truth_boxes: List of ground truth boxes in YOLO format\n",
    "                            [class_id, confidence, x_center, y_center, width, height].\n",
    "        predicted_boxes: List of predicted boxes in YOLO format\n",
    "                         [class_id, confidence, x_center, y_center, width, height].\n",
    "        class_mapping: Dictionary mapping class IDs to class names.\n",
    "    \"\"\"\n",
    "    # Invert the class mapping for easier access to class names\n",
    "    inverted_class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "\n",
    "    # Convert the image to a NumPy array and get its dimensions\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.permute(1, 2, 0).cpu().numpy()\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Plot ground truth boxes (green)\n",
    "    for box in ground_truth_boxes:\n",
    "        label_index, box = box[0], box[2:]\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * width, upper_left_y * height),\n",
    "            box[2] * width,\n",
    "            box[3] * height,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"green\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        class_name = inverted_class_mapping.get(label_index, \"Unknown\")\n",
    "        ax.text(\n",
    "            upper_left_x * width,\n",
    "            upper_left_y * height,\n",
    "            class_name,\n",
    "            color=\"white\",\n",
    "            fontsize=12,\n",
    "            bbox=dict(facecolor=\"green\", alpha=0.2),\n",
    "        )\n",
    "\n",
    "    # Plot predicted boxes (red)\n",
    "    for box in predicted_boxes:\n",
    "        label_index, box = box[0], box[2:]\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * width, upper_left_y * height),\n",
    "            box[2] * width,\n",
    "            box[3] * height,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"red\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        class_name = inverted_class_mapping.get(label_index, \"Unknown\")\n",
    "        ax.text(\n",
    "            upper_left_x * width,\n",
    "            upper_left_y * height,\n",
    "            class_name,\n",
    "            color=\"white\",\n",
    "            fontsize=12,\n",
    "            bbox=dict(facecolor=\"red\", alpha=0.2),\n",
    "        )\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6426ae87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T15:34:19.867974Z",
     "iopub.status.busy": "2025-01-21T15:34:19.867742Z",
     "iopub.status.idle": "2025-01-21T15:34:19.873630Z",
     "shell.execute_reply": "2025-01-21T15:34:19.872849Z"
    },
    "papermill": {
     "duration": 0.011503,
     "end_time": "2025-01-21T15:34:19.874812",
     "exception": false,
     "start_time": "2025-01-21T15:34:19.863309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    Test the YOLO model on the validation set and visualize predictions.\n",
    "    \"\"\"\n",
    "    # Initialize the YOLO model\n",
    "    model = Yolov1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n",
    "\n",
    "    # Load pre-trained model weights\n",
    "    if LOAD_MODEL:\n",
    "        model.load_state_dict(torch.load(LOAD_MODEL_FILE)[\"state_dict\"])\n",
    "\n",
    "    # Prepare test dataset and DataLoader\n",
    "    test_dataset = CustomVOCDataset(root='./data', image_set='val', download=False)\n",
    "    test_dataset.init_config_yolo(class_mapping=class_mapping, custom_transforms=get_valid_transforms())\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Process the first batch from the test loader\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(DEVICE)\n",
    "            out = model(x)\n",
    "\n",
    "            # Convert outputs and ground truth to bounding boxes\n",
    "            pred_bboxes = cellboxes_to_boxes(out)\n",
    "            gt_bboxes = cellboxes_to_boxes(y)\n",
    "\n",
    "            # Visualize the first 8 images with bounding boxes\n",
    "            for idx in range(min(8, len(x))):\n",
    "                pred_box = non_max_suppression(\n",
    "                    pred_bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\"\n",
    "                )\n",
    "                gt_box = non_max_suppression(\n",
    "                    gt_bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\"\n",
    "                )\n",
    "                image = x[idx].permute(1, 2, 0).cpu() / 255.0\n",
    "                plot_image_with_labels(image, gt_box, pred_box, class_mapping)\n",
    "\n",
    "            break  # Process only the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f05b63e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T15:34:19.883331Z",
     "iopub.status.busy": "2025-01-21T15:34:19.883117Z",
     "iopub.status.idle": "2025-01-21T15:39:15.125819Z",
     "shell.execute_reply": "2025-01-21T15:39:15.124376Z"
    },
    "papermill": {
     "duration": 295.248455,
     "end_time": "2025-01-21T15:39:15.127226",
     "exception": true,
     "start_time": "2025-01-21T15:34:19.878771",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to ./data/VOCtrainval_11-May-2012.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.00G/2.00G [04:26<00:00, 7.51MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n",
      "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
      "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [16, 7, 7, 2]], which is output 0 of AsStridedBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2da0ffaf5447>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-43f783d631d9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# Train and evaluate on train, validation, and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mtrain_mAP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mval_mAP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Using test_fn for validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtest_mAP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-37bfc2fff1e6>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(train_loader, model, optimizer, loss_fn, epoch)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [16, 7, 7, 2]], which is output 0 of AsStridedBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e54f73f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-21T15:33:51.016607Z",
     "iopub.status.idle": "2025-01-21T15:33:51.016894Z",
     "shell.execute_reply": "2025-01-21T15:33:51.016788Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a90896",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 309.835882,
   "end_time": "2025-01-21T15:39:17.149069",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-21T15:34:07.313187",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
