{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "HItsinp0SFqA",
      "metadata": {
        "id": "HItsinp0SFqA"
      },
      "source": [
        "## **0. Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bb96f0ec-3525-4e79-9ad1-8e95c15a9cf2",
      "metadata": {
        "id": "bb96f0ec-3525-4e79-9ad1-8e95c15a9cf2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision\n",
        "from torchvision.datasets import FashionMNIST\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "SEED = 42\n",
        "set_seed(SEED)"
      ],
      "metadata": {
        "id": "P3ehfc2p0vqq"
      },
      "id": "P3ehfc2p0vqq",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Prepare dataset**\n"
      ],
      "metadata": {
        "id": "Mw6hLiNAugrD"
      },
      "id": "Mw6hLiNAugrD"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c73f1e96-b8ea-4b8f-b4f3-9b3f27b42acd",
      "metadata": {
        "id": "c73f1e96-b8ea-4b8f-b4f3-9b3f27b42acd",
        "outputId": "eb50f717-1953-4d31-f409-059e9a31d7fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:03<00:00, 8.71MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 138kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 2.53MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 20.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Train size: 54000\n",
            "Validation size: 6000\n",
            "Test size: 10000\n"
          ]
        }
      ],
      "source": [
        "batch_size = 512\n",
        "\n",
        "train_dataset = FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "train_ratio = 0.9\n",
        "train_size = int(len(train_dataset) * train_ratio)\n",
        "val_size = len(train_dataset) - train_size\n",
        "\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Train size: {len(train_subset)}\")\n",
        "print(f\"Validation size: {len(val_subset)}\")\n",
        "print(f\"Test size: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75eeaa46",
      "metadata": {
        "id": "75eeaa46"
      },
      "source": [
        "## **2. Build MLP network**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "589bb0df",
      "metadata": {
        "id": "589bb0df"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
        "        super(MLP, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.input_layer = nn.Linear(input_dims, hidden_dims)\n",
        "        self.hidden_layers = nn.ModuleList(\n",
        "            [nn.Linear(hidden_dims, hidden_dims) for _ in range(7)]\n",
        "        )\n",
        "        self.output_layer = nn.Linear(hidden_dims, output_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.input_layer(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            x = hidden_layer(x)\n",
        "            x = nn.Sigmoid()(x)\n",
        "\n",
        "        x = self.output_layer(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0db5beca-ee90-4d2e-bb2f-62b20da68cf3",
      "metadata": {
        "id": "0db5beca-ee90-4d2e-bb2f-62b20da68cf3"
      },
      "outputs": [],
      "source": [
        "input_dims = 784\n",
        "hidden_dims = 128\n",
        "output_dims = 10\n",
        "lr = 1e-2\n",
        "\n",
        "model = MLP(input_dims=input_dims,\n",
        "            hidden_dims=hidden_dims,\n",
        "            output_dims=output_dims).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Training**"
      ],
      "metadata": {
        "id": "lBS7q-JzwFgC"
      },
      "id": "lBS7q-JzwFgC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21211483-aeed-4beb-aeea-d1e58ae7baf9",
      "metadata": {
        "id": "21211483-aeed-4beb-aeea-d1e58ae7baf9",
        "outputId": "b776655f-68eb-4577-ae73-2e594741b6ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1/100, Train_Loss: 2.3058, Train_Acc: 0.0970, Validation Loss: 1.3817, Val_Acc: 0.1003\n",
            "EPOCH 2/100, Train_Loss: 2.3028, Train_Acc: 0.1000, Validation Loss: 1.3817, Val_Acc: 0.0997\n",
            "EPOCH 3/100, Train_Loss: 2.3028, Train_Acc: 0.0970, Validation Loss: 1.3817, Val_Acc: 0.0895\n",
            "EPOCH 4/100, Train_Loss: 2.3027, Train_Acc: 0.1009, Validation Loss: 1.3817, Val_Acc: 0.1022\n",
            "EPOCH 5/100, Train_Loss: 2.3028, Train_Acc: 0.0982, Validation Loss: 1.3816, Val_Acc: 0.0895\n",
            "EPOCH 6/100, Train_Loss: 2.3028, Train_Acc: 0.0991, Validation Loss: 1.3816, Val_Acc: 0.1022\n",
            "EPOCH 7/100, Train_Loss: 2.3028, Train_Acc: 0.0984, Validation Loss: 1.3818, Val_Acc: 0.0895\n",
            "EPOCH 8/100, Train_Loss: 2.3027, Train_Acc: 0.0981, Validation Loss: 1.3817, Val_Acc: 0.1018\n",
            "EPOCH 9/100, Train_Loss: 2.3028, Train_Acc: 0.0985, Validation Loss: 1.3819, Val_Acc: 0.0895\n",
            "EPOCH 10/100, Train_Loss: 2.3028, Train_Acc: 0.1010, Validation Loss: 1.3817, Val_Acc: 0.1022\n",
            "EPOCH 11/100, Train_Loss: 2.3028, Train_Acc: 0.0998, Validation Loss: 1.3817, Val_Acc: 0.1020\n",
            "EPOCH 12/100, Train_Loss: 2.3028, Train_Acc: 0.0992, Validation Loss: 1.3816, Val_Acc: 0.1000\n",
            "EPOCH 13/100, Train_Loss: 2.3028, Train_Acc: 0.1004, Validation Loss: 1.3818, Val_Acc: 0.0895\n",
            "EPOCH 14/100, Train_Loss: 2.3028, Train_Acc: 0.0976, Validation Loss: 1.3817, Val_Acc: 0.0937\n",
            "EPOCH 15/100, Train_Loss: 2.3027, Train_Acc: 0.1017, Validation Loss: 1.3817, Val_Acc: 0.1018\n",
            "EPOCH 16/100, Train_Loss: 2.3028, Train_Acc: 0.0994, Validation Loss: 1.3817, Val_Acc: 0.1020\n",
            "EPOCH 17/100, Train_Loss: 2.3028, Train_Acc: 0.0983, Validation Loss: 1.3818, Val_Acc: 0.0937\n",
            "EPOCH 18/100, Train_Loss: 2.3028, Train_Acc: 0.1002, Validation Loss: 1.3816, Val_Acc: 0.0937\n",
            "EPOCH 19/100, Train_Loss: 2.3028, Train_Acc: 0.1008, Validation Loss: 1.3817, Val_Acc: 0.0895\n",
            "EPOCH 20/100, Train_Loss: 2.3028, Train_Acc: 0.0994, Validation Loss: 1.3816, Val_Acc: 0.1012\n",
            "EPOCH 21/100, Train_Loss: 2.3028, Train_Acc: 0.1001, Validation Loss: 1.3819, Val_Acc: 0.0895\n",
            "EPOCH 22/100, Train_Loss: 2.3027, Train_Acc: 0.0997, Validation Loss: 1.3821, Val_Acc: 0.0895\n",
            "EPOCH 23/100, Train_Loss: 2.3028, Train_Acc: 0.0979, Validation Loss: 1.3819, Val_Acc: 0.0997\n",
            "EPOCH 24/100, Train_Loss: 2.3028, Train_Acc: 0.0985, Validation Loss: 1.3819, Val_Acc: 0.0937\n",
            "EPOCH 25/100, Train_Loss: 2.3028, Train_Acc: 0.1001, Validation Loss: 1.3817, Val_Acc: 0.1020\n",
            "EPOCH 26/100, Train_Loss: 2.3028, Train_Acc: 0.0995, Validation Loss: 1.3819, Val_Acc: 0.0937\n",
            "EPOCH 27/100, Train_Loss: 2.3028, Train_Acc: 0.0989, Validation Loss: 1.3818, Val_Acc: 0.0997\n",
            "EPOCH 28/100, Train_Loss: 2.3028, Train_Acc: 0.1005, Validation Loss: 1.3815, Val_Acc: 0.1022\n",
            "EPOCH 29/100, Train_Loss: 2.3028, Train_Acc: 0.0983, Validation Loss: 1.3819, Val_Acc: 0.0937\n",
            "EPOCH 30/100, Train_Loss: 2.3028, Train_Acc: 0.0993, Validation Loss: 1.3817, Val_Acc: 0.0937\n",
            "EPOCH 31/100, Train_Loss: 2.3027, Train_Acc: 0.0994, Validation Loss: 1.3817, Val_Acc: 0.0937\n",
            "EPOCH 32/100, Train_Loss: 2.3028, Train_Acc: 0.0999, Validation Loss: 1.3817, Val_Acc: 0.1022\n",
            "EPOCH 33/100, Train_Loss: 2.3028, Train_Acc: 0.0992, Validation Loss: 1.3819, Val_Acc: 0.1020\n",
            "EPOCH 34/100, Train_Loss: 2.3028, Train_Acc: 0.1000, Validation Loss: 1.3818, Val_Acc: 0.0895\n",
            "EPOCH 35/100, Train_Loss: 2.3028, Train_Acc: 0.0996, Validation Loss: 1.3817, Val_Acc: 0.0997\n",
            "EPOCH 36/100, Train_Loss: 2.3028, Train_Acc: 0.0987, Validation Loss: 1.3818, Val_Acc: 0.1012\n",
            "EPOCH 37/100, Train_Loss: 2.3028, Train_Acc: 0.1015, Validation Loss: 1.3818, Val_Acc: 0.0937\n",
            "EPOCH 38/100, Train_Loss: 2.3028, Train_Acc: 0.1007, Validation Loss: 1.3818, Val_Acc: 0.1020\n",
            "EPOCH 39/100, Train_Loss: 2.3028, Train_Acc: 0.0961, Validation Loss: 1.3820, Val_Acc: 0.0895\n",
            "EPOCH 40/100, Train_Loss: 2.3028, Train_Acc: 0.1009, Validation Loss: 1.3817, Val_Acc: 0.1022\n",
            "EPOCH 41/100, Train_Loss: 2.3028, Train_Acc: 0.0993, Validation Loss: 1.3820, Val_Acc: 0.0937\n",
            "EPOCH 42/100, Train_Loss: 2.3028, Train_Acc: 0.1004, Validation Loss: 1.3817, Val_Acc: 0.1003\n",
            "EPOCH 43/100, Train_Loss: 2.3028, Train_Acc: 0.0986, Validation Loss: 1.3818, Val_Acc: 0.0895\n",
            "EPOCH 44/100, Train_Loss: 2.3028, Train_Acc: 0.1002, Validation Loss: 1.3817, Val_Acc: 0.1003\n",
            "EPOCH 45/100, Train_Loss: 2.3028, Train_Acc: 0.0983, Validation Loss: 1.3818, Val_Acc: 0.0937\n",
            "EPOCH 46/100, Train_Loss: 2.3028, Train_Acc: 0.0985, Validation Loss: 1.3818, Val_Acc: 0.1018\n",
            "EPOCH 47/100, Train_Loss: 2.3028, Train_Acc: 0.0987, Validation Loss: 1.3817, Val_Acc: 0.0997\n",
            "EPOCH 48/100, Train_Loss: 2.3028, Train_Acc: 0.0981, Validation Loss: 1.3818, Val_Acc: 0.0895\n",
            "EPOCH 49/100, Train_Loss: 2.3028, Train_Acc: 0.0997, Validation Loss: 1.3817, Val_Acc: 0.0937\n",
            "EPOCH 50/100, Train_Loss: 2.3028, Train_Acc: 0.0999, Validation Loss: 1.3815, Val_Acc: 0.1018\n"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "train_loss_lst = []\n",
        "train_acc_lst = []\n",
        "val_loss_lst = []\n",
        "val_acc_lst = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    count = 0\n",
        "\n",
        "    model.train()\n",
        "    for X_train, y_train in train_loader:\n",
        "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (torch.argmax(outputs, 1) == y_train).sum().item()\n",
        "        count += len(y_train)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_loss_lst.append(train_loss)\n",
        "    train_acc /= count\n",
        "    train_acc_lst.append(train_acc)\n",
        "\n",
        "    val_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    count = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X_val, y_val in val_loader:\n",
        "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "            outputs = model(X_val)\n",
        "            loss = criterion(outputs, y_val)\n",
        "            val_loss += loss.item()\n",
        "            val_acc += (torch.argmax(outputs, 1) == y_val).sum().item()\n",
        "            count += len(y_val)\n",
        "\n",
        "    val_loss /= len(test_loader)\n",
        "    val_loss_lst.append(val_loss)\n",
        "    val_acc /= count\n",
        "    val_acc_lst.append(val_acc)\n",
        "\n",
        "    print(f\"EPOCH {epoch+1}/{epochs}, Train_Loss: {train_loss:.4f}, Train_Acc: {train_acc:.4f}, Validation Loss: {val_loss:.4f}, Val_Acc: {val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "321d7070-b736-4ebb-94fa-59cd37ff50b3",
      "metadata": {
        "id": "321d7070-b736-4ebb-94fa-59cd37ff50b3"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
        "ax[0, 0].plot(train_loss_lst, color='green')\n",
        "ax[0, 0].set(xlabel='Epoch', ylabel='Loss')\n",
        "ax[0, 0].set_title('Training Loss')\n",
        "\n",
        "ax[0, 1].plot(val_loss_lst, color='orange')\n",
        "ax[0, 1].set(xlabel='Epoch', ylabel='Loss')\n",
        "ax[0, 1].set_title('Validation Loss')\n",
        "\n",
        "ax[1, 0].plot(train_acc_lst, color='green')\n",
        "ax[1, 0].set(xlabel='Epoch', ylabel='Accuracy')\n",
        "ax[1, 0].set_title('Training Accuracy')\n",
        "\n",
        "ax[1, 1].plot(val_acc_lst, color='orange')\n",
        "ax[1, 1].set(xlabel='Epoch', ylabel='Accuracy')\n",
        "ax[1, 1].set_title('Validation Accuracy')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Evaluation**"
      ],
      "metadata": {
        "id": "CY9OpDyiPL2V"
      },
      "id": "CY9OpDyiPL2V"
    },
    {
      "cell_type": "code",
      "source": [
        "val_target = []\n",
        "val_predict = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for X_val, y_val in val_loader:\n",
        "        X_val = X_val.to(device)\n",
        "        y_val = y_val.to(device)\n",
        "        outputs = model(X_val)\n",
        "\n",
        "        val_predict.append(outputs.cpu())\n",
        "        val_target.append(y_val.cpu())\n",
        "\n",
        "    val_predict = torch.cat(val_predict)\n",
        "    val_target = torch.cat(val_target)\n",
        "    val_acc = (torch.argmax(val_predict, 1) == val_target).sum().item() / len(val_target)\n",
        "\n",
        "    print('Evaluation on val set:')\n",
        "    print(f'Accuracy: {val_acc}')"
      ],
      "metadata": {
        "id": "VC8cygPWPKy6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "VC8cygPWPKy6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Improve"
      ],
      "metadata": {
        "id": "N0Vspcg52SPj"
      },
      "id": "N0Vspcg52SPj"
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNormalization(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNormalization, self).__init__()\n",
        "\n",
        "    def forward(self,x):\n",
        "        mean = torch.mean(x)\n",
        "        std = torch.std(x)\n",
        "        x = (x - mean) / std\n",
        "        return x"
      ],
      "metadata": {
        "id": "-66TFsme3FWw"
      },
      "id": "-66TFsme3FWw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
        "        super(MLP, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.input_layer = nn.Linear(input_dims, hidden_dims)\n",
        "        self.hidden_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dims, hidden_dims),\n",
        "                nn.ReLU(),\n",
        "                MyNormalization()\n",
        "            )\n",
        "            for _ in range(7)\n",
        "        ])\n",
        "        self.output_layer = nn.Linear(hidden_dims, output_dims)\n",
        "\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.normal_(module.weight, mean=0.0, std=10.0)\n",
        "                nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.input_layer(x)\n",
        "        x = MyNormalization()(x)\n",
        "        x = nn.ReLU()(x)\n",
        "\n",
        "        skip = x\n",
        "\n",
        "        for i, layer in enumerate(self.hidden_layers):\n",
        "            x = layer(x)\n",
        "            if (i + 1) % 2 == 0:\n",
        "                x = x + skip\n",
        "                skip = x\n",
        "\n",
        "        out = self.output_layer(x)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "wxqrVVa82OoY"
      },
      "id": "wxqrVVa82OoY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dims = 784\n",
        "hidden_dims = 128\n",
        "output_dims = 10\n",
        "lr = 1e-3\n",
        "\n",
        "model = MLP(input_dims=input_dims,\n",
        "            hidden_dims=hidden_dims,\n",
        "            output_dims=output_dims).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "h6IhUnRY26JD"
      },
      "id": "h6IhUnRY26JD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_1layer(nn.Module):\n",
        "    def __init__(self, input_dims, output_dims):\n",
        "        super(MLP_1layer, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dims, output_dims)\n",
        "\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.normal_(module.weight, mean=0.0, std=0.05)\n",
        "                nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = nn.Flatten()(x)\n",
        "        x = self.layer1(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        return x\n",
        "\n",
        "class MLP_2layer(nn.Module):\n",
        "    def __init__(self, input_dims, output_dims):\n",
        "        super(MLP_2layer, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dims, output_dims)\n",
        "        self.layer2 = nn.Linear(hidden_dims, output_dims)\n",
        "\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.normal_(module.weight, mean=0.0, std=0.05)\n",
        "                nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = nn.Flatten()(x)\n",
        "        x = self.layer1(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer2(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "q5--tYDs44aM"
      },
      "id": "q5--tYDs44aM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first = MLP_2layer(input_dims=784, output_dims=128)\n",
        "second = MLP_2layer(input_dims=128, output_dims=128)\n",
        "third = MLP_2layer(input_dims=128, output_dims=128)\n",
        "fourth = MLP_1layer(input_dims=128, output_dims=128)\n",
        "lr = 1e-2\n",
        "loss_fn = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "pND62Hgs5jDq"
      },
      "id": "pND62Hgs5jDq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(first,\n",
        "                      nn.Linear(128,10)).to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "epochs = 100\n",
        "train_loss_lst = []\n",
        "train_acc_lst = []\n",
        "val_loss_lst = []\n",
        "val_acc_lst = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    count = 0\n",
        "\n",
        "    model.train()\n",
        "    for X_train, y_train in train_loader:\n",
        "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (torch.argmax(outputs, 1) == y_train).sum().item()\n",
        "        count += len(y_train)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_loss_lst.append(train_loss)\n",
        "    train_acc /= count\n",
        "    train_acc_lst.append(train_acc)\n",
        "\n",
        "    val_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    count = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X_val, y_val in val_loader:\n",
        "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "            outputs = model(X_val)\n",
        "            loss = criterion(outputs, y_val)\n",
        "            val_loss += loss.item()\n",
        "            val_acc += (torch.argmax(outputs, 1) == y_val).sum().item()\n",
        "            count += len(y_val)\n",
        "\n",
        "    val_loss /= len(test_loader)\n",
        "    val_loss_lst.append(val_loss)\n",
        "    val_acc /= count\n",
        "    val_acc_lst.append(val_acc)\n",
        "\n",
        "    print(f\"EPOCH {epoch+1}/{epochs}, Train_Loss: {train_loss:.4f}, Train_Acc: {train_acc:.4f}, Validation Loss: {val_loss:.4f}, Val_Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "PVjGZRTC6L-N"
      },
      "id": "PVjGZRTC6L-N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in first.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model = nn.Sequential(first,\n",
        "                      second,\n",
        "                      nn.Linear(128,10)).to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "epochs = 100\n",
        "train_loss_lst = []\n",
        "train_acc_lst = []\n",
        "val_loss_lst = []\n",
        "val_acc_lst = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    count = 0\n",
        "\n",
        "    model.train()\n",
        "    for X_train, y_train in train_loader:\n",
        "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (torch.argmax(outputs, 1) == y_train).sum().item()\n",
        "        count += len(y_train)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_loss_lst.append(train_loss)\n",
        "    train_acc /= count\n",
        "    train_acc_lst.append(train_acc)\n",
        "\n",
        "    val_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    count = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X_val, y_val in val_loader:\n",
        "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "            outputs = model(X_val)\n",
        "            loss = criterion(outputs, y_val)\n",
        "            val_loss += loss.item()\n",
        "            val_acc += (torch.argmax(outputs, 1) == y_val).sum().item()\n",
        "            count += len(y_val)\n",
        "\n",
        "    val_loss /= len(test_loader)\n",
        "    val_loss_lst.append(val_loss)\n",
        "    val_acc /= count\n",
        "    val_acc_lst.append(val_acc)\n",
        "\n",
        "    print(f\"EPOCH {epoch+1}/{epochs}, Train_Loss: {train_loss:.4f}, Train_Acc: {train_acc:.4f}, Validation Loss: {val_loss:.4f}, Val_Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "yxnd5IPG6i0y"
      },
      "id": "yxnd5IPG6i0y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in first.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in second.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model = nn.Sequential(first,\n",
        "                      second,\n",
        "                      third,\n",
        "                      nn.Linear(128,10)).to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "epochs = 100\n",
        "train_loss_lst = []\n",
        "train_acc_lst = []\n",
        "val_loss_lst = []\n",
        "val_acc_lst = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    count = 0\n",
        "\n",
        "    model.train()\n",
        "    for X_train, y_train in train_loader:\n",
        "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (torch.argmax(outputs, 1) == y_train).sum().item()\n",
        "        count += len(y_train)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_loss_lst.append(train_loss)\n",
        "    train_acc /= count\n",
        "    train_acc_lst.append(train_acc)\n",
        "\n",
        "    val_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    count = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X_val, y_val in val_loader:\n",
        "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "            outputs = model(X_val)\n",
        "            loss = criterion(outputs, y_val)\n",
        "            val_loss += loss.item()\n",
        "            val_acc += (torch.argmax(outputs, 1) == y_val).sum().item()\n",
        "            count += len(y_val)\n",
        "\n",
        "    val_loss /= len(test_loader)\n",
        "    val_loss_lst.append(val_loss)\n",
        "    val_acc /= count\n",
        "    val_acc_lst.append(val_acc)\n",
        "\n",
        "    print(f\"EPOCH {epoch+1}/{epochs}, Train_Loss: {train_loss:.4f}, Train_Acc: {train_acc:.4f}, Validation Loss: {val_loss:.4f}, Val_Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "ciOB98ky605I"
      },
      "id": "ciOB98ky605I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in first.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in second.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in third.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "model = nn.Sequential(first,\n",
        "                      second,\n",
        "                      third,\n",
        "                      fourth,\n",
        "                      nn.Linear(128,10)).to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "epochs = 100\n",
        "train_loss_lst = []\n",
        "train_acc_lst = []\n",
        "val_loss_lst = []\n",
        "val_acc_lst = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    count = 0\n",
        "\n",
        "    model.train()\n",
        "    for X_train, y_train in train_loader:\n",
        "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (torch.argmax(outputs, 1) == y_train).sum().item()\n",
        "        count += len(y_train)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_loss_lst.append(train_loss)\n",
        "    train_acc /= count\n",
        "    train_acc_lst.append(train_acc)\n",
        "\n",
        "    val_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    count = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X_val, y_val in val_loader:\n",
        "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "            outputs = model(X_val)\n",
        "            loss = criterion(outputs, y_val)\n",
        "            val_loss += loss.item()\n",
        "            val_acc += (torch.argmax(outputs, 1) == y_val).sum().item()\n",
        "            count += len(y_val)\n",
        "\n",
        "    val_loss /= len(test_loader)\n",
        "    val_loss_lst.append(val_loss)\n",
        "    val_acc /= count\n",
        "    val_acc_lst.append(val_acc)\n",
        "\n",
        "    print(f\"EPOCH {epoch+1}/{epochs}, Train_Loss: {train_loss:.4f}, Train_Acc: {train_acc:.4f}, Validation Loss: {val_loss:.4f}, Val_Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "ZJ1f2qjo7EZD"
      },
      "id": "ZJ1f2qjo7EZD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientNormalization(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        ctx.save_for_backward(input)\n",
        "        return input\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        mean = torch.mean(grad_output)\n",
        "        std = torch.std(grad_output)\n",
        "        grad_input = (grad_output - mean) / (std + 1e-6)\n",
        "        return grad_input\n",
        "\n",
        "class GradientNormalizationLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GradientNormalizationLayer, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return GradientNormalization.apply(x)\n",
        ""
      ],
      "metadata": {
        "id": "Bno7fKVb7R-B"
      },
      "id": "Bno7fKVb7R-B",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}